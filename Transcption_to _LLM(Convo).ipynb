{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Transcription and LLM Query Notebook\n",
    "\n",
    "This notebook demonstrates a real-time transcription system integrated with a language model (LLM) for generating responses. The notebook covers:\n",
    "\n",
    "1. **Setting Up Dependencies**\n",
    "2. **Defining Helper Functions**\n",
    "3. **Main Transcription and LLM Query Workflow**\n",
    "4. **Running the Transcription and LLM Integration**\n",
    "5. **Saving Results to File**\n",
    "\n",
    "Let's start by setting up our environment and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import speech_recognition as sr\n",
    "import whisper\n",
    "import torch\n",
    "import webrtcvad\n",
    "from datetime import datetime, timedelta\n",
    "from queue import Queue\n",
    "from time import sleep\n",
    "from huggingface_hub import InferenceClient\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Helper Functions\n",
    "\n",
    "First, we will define the helper functions used in the script, including functions for querying the LLM and saving results to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(text, client):\n",
    "    response = \"\"\n",
    "    for message in client.chat_completion(\n",
    "        messages=[{\"role\": \"user\", \"content\": text}],\n",
    "        max_tokens=500,\n",
    "        stream=True,\n",
    "    ):\n",
    "        response += message.choices[0].delta.content\n",
    "    \n",
    "    # Limit the response to 2 sentences\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', response)\n",
    "    limited_response = ' '.join(sentences[:2])\n",
    "    return limited_response\n",
    "\n",
    "def save_to_file(transcriptions, responses, durations):\n",
    "    # Create the responses directory if it does not exist\n",
    "    if not os.path.exists('responses'):\n",
    "        os.makedirs('responses')\n",
    "\n",
    "    # Generate filename with date and time\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    filename = f\"responses/{timestamp}_transcriptions_responses.txt\"\n",
    "\n",
    "    # Write to file\n",
    "    with open(filename, 'w') as f:\n",
    "        for i in range(len(transcriptions)):\n",
    "            f.write(f\"Transcription [{i+1}]:\\n\")\n",
    "            f.write(f\"{transcriptions[i]}\\n\")\n",
    "            f.write(f\"Response [{i+1}]:\\n\")\n",
    "            f.write(f\"{responses[i]}\\n\")\n",
    "            f.write(f\"Processing Time: {durations[i]:.4f}s\\n\")\n",
    "            f.write(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Transcription and LLM Workflow\n",
    "\n",
    "In this section, we initialize the transcription system, the LLM client, and the WebRTC VAD. We also define the real-time transcription process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Define and parse arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model\", default=\"base\", choices=[\"tiny\", \"base\", \"small\", \"medium\", \"large\"])\n",
    "    parser.add_argument(\"--non_english\", action='store_true')\n",
    "    parser.add_argument(\"--energy_threshold\", default=1000, type=int)\n",
    "    parser.add_argument(\"--record_timeout\", default=1, type=float)\n",
    "    parser.add_argument(\"--phrase_timeout\", default=1.5, type=float)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    phrase_time = None\n",
    "    data_queue = Queue()\n",
    "    recorder = sr.Recognizer()\n",
    "    recorder.energy_threshold = args.energy_threshold\n",
    "    recorder.dynamic_energy_threshold = False\n",
    "\n",
    "    source = sr.Microphone(sample_rate=16000)\n",
    "\n",
    "    model = args.model\n",
    "    if args.model != \"large\" and not args.non_english:\n",
    "        model = model + \".en\"\n",
    "    audio_model = whisper.load_model(model)\n",
    "\n",
    "    record_timeout = args.record_timeout\n",
    "    phrase_timeout = args.phrase_timeout\n",
    "\n",
    "    transcription = []\n",
    "    responses = []\n",
    "    durations = []\n",
    "\n",
    "    # Initialize WebRTC VAD\n",
    "    vad = webrtcvad.Vad(1)  # Mode 1 is a good balance for sensitivity\n",
    "\n",
    "    # Initialize the Hugging Face Inference Client\n",
    "    client = InferenceClient(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        token=\"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\",\n",
    "    )\n",
    "\n",
    "    def record_callback(_, audio: sr.AudioData):\n",
    "        data = audio.get_raw_data()\n",
    "        data_queue.put(data)\n",
    "\n",
    "    recorder.listen_in_background(source, record_callback, phrase_time_limit=record_timeout)\n",
    "\n",
    "    print(\"Model loaded.\\n\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            now = datetime.utcnow()\n",
    "\n",
    "            if not data_queue.empty():\n",
    "                phrase_complete = False\n",
    "\n",
    "                if phrase_time and now - phrase_time > timedelta(seconds=phrase_timeout):\n",
    "                    phrase_complete = True\n",
    "\n",
    "                phrase_time = now\n",
    "\n",
    "                # Process each frame of audio data\n",
    "                while not data_queue.empty():\n",
    "                    audio_data = data_queue.get()\n",
    "\n",
    "                    # Convert raw PCM data to a numpy array\n",
    "                    audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "\n",
    "                    # Convert float32 to int16 for VAD processing\n",
    "                    audio_int16 = (audio_np * 32767).astype(np.int16)\n",
    "                    audio_pcm = audio_int16.tobytes()\n",
    "\n",
    "                    # Apply VAD\n",
    "                    frame_duration_ms = 30\n",
    "                    sample_rate = 16000\n",
    "                    frame_size = int(sample_rate * frame_duration_ms / 1000)\n",
    "                    num_frames = len(audio_pcm) // (frame_size * 2)\n",
    "\n",
    "                    contains_speech = False\n",
    "                    for i in range(num_frames):\n",
    "                        frame = audio_pcm[i * frame_size * 2:(i + 1) * frame_size * 2]\n",
    "                        if vad.is_speech(frame, sample_rate=sample_rate):\n",
    "                            contains_speech = True\n",
    "                            break\n",
    "\n",
    "                    if not contains_speech:\n",
    "                        continue\n",
    "\n",
    "                    # Track the start time\n",
    "                    start_time = datetime.now()\n",
    "\n",
    "                    # Transcribe the audio frame\n",
    "                    result = audio_model.transcribe(audio_np, fp16=torch.cuda.is_available())\n",
    "                    text = result['text'].strip()\n",
    "\n",
    "                    # Track the end time\n",
    "                    end_time = datetime.now()\n",
    "\n",
    "                    # Calculate the time taken for processing\n",
    "                    duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "                    # Append the new transcription to the list\n",
    "                    transcription.append(f\"[{duration:.4f}s] {text}\")\n",
    "\n",
    "                    # Clear the console and print the updated transcription\n",
    "                    os.system('cls' if os.name == 'nt' else 'clear')\n",
    "                    print(\"\\n\".join(transcription))\n",
    "                    print('', end='', flush=True)\n",
    "\n",
    "                    # Query LLM with the transcribed text\n",
    "                    response = query_llm(text, client)\n",
    "                    responses.append(response)\n",
    "                    durations.append(duration)\n",
    "                    \n",
    "                    print(\"\\nLLM Response:\")\n",
    "                    print(response)\n",
    "\n",
    "            else:\n",
    "                sleep(0.05)  # Reduced sleep time for improved responsiveness\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "    # Save to file after exiting the loop\n",
    "    save_to_file(transcription, responses, durations)\n",
    "\n",
    "    print(\"\\n\\nFinal Transcription:\")\n",
    "    print(\"\\n\".join(transcription))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run the Transcription and LLM Integration\n",
    "\n",
    "In this section, we call the `main()` function to execute the transcription and LLM integration process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the main function to start the transcription and LLM integration process\n",
    "# use keyboard Interrupt to exit the module\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
