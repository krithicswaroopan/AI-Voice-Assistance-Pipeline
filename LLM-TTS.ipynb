{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Advanced NLP with Transformers and Hugging Face\n",
       "\n",
       "This notebook demonstrates how to use various models and tools from the Hugging Face ecosystem for natural language processing (NLP) tasks. The notebook covers:\n",
       "\n",
       "1. **Loading and Using a Transformer Model**\n",
       "2. **Performing Inference with Mixed Precision**\n",
       "3. **Generating Responses Using a Transformer Model**\n",
       "4. **Using the Hugging Face Inference API**\n",
       "5. **Using Microsoft TTS API for Text-to-Speech**\n",
       "\n",
       "Let's start by setting up our environment."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Importing necessary libraries\n",
       "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
       "import torch\n",
       "from torch.cuda.amp import autocast\n",
       "import requests\n",
       "from IPython.display import Audio"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Load Tokenizer and Model\n",
       "First, we'll load a pre-trained tokenizer and model from the Hugging Face model hub. In this case, we use the `mistralai/Mistral-7B-v0.1` model."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Load tokenizer and model\n",
       "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
       "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
       "\n",
       "# Move model to GPU\n",
       "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
       "model.to(device)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Perform Inference with Mixed Precision\n",
       "To improve performance and reduce memory usage, we can use mixed precision inference. We define a function `infer` to tokenize the input text and perform inference using mixed precision."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
       "def infer(input_text):\n",
       "    # Tokenize input\n",
       "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
       "    \n",
       "    # Perform inference with mixed precision\n",
       "    with autocast():\n",
       "        outputs = model(**inputs)\n",
       "    \n",
       "    # Clear GPU cache\n",
       "    torch.cuda.empty_cache()\n",
       "    \n",
       "    # Process outputs\n",
       "    return outputs\n",
       "\n",
       "# Example usage\n",
       "input_text = \"Your input text here\"\n",
       "outputs = infer(input_text)\n",
       "print(outputs)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Generate Responses Using Transformer Model\n",
       "We also define a function `generate_response` to generate responses based on a prompt. This function uses the `generate` method of the model and decodes the generated text."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
       "def generate_response(prompt):\n",
       "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
       "    outputs = model.generate(\n",
       "        inputs.input_ids,\n",
       "        max_length=100,\n",
       "        num_return_sequences=1,\n",
       "        pad_token_id=tokenizer.eos_token_id,\n",
       "    )\n",
       "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
       "\n",
       "# Example query\n",
       "query = \"What is the impact of AI on modern healthcare?\"\n",
       "response = generate_response(query)\n",
       "print(response)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Use the Hugging Face Inference API\n",
       "We demonstrate how to use the Hugging Face Inference API for a model named `mistralai/Mistral-7B-Instruct-v0.1` to handle chat completions. This part requires an API token for authentication."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stdout",
        "text": [
         "Who is obama?\n"
        ]
       }
      ],
      "source": [
       "from huggingface_hub import InferenceClient\n",
       "\n",
       "client = InferenceClient(\n",
       "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
       "    token=\"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\",\n",
       ")\n",
       "\n",
       "for message in client.chat_completion(\n",
       "    messages=[{\"role\": \"user\", \"content\": \"Who is obama?\"}],\n",
       "    max_tokens=500,\n",
       "    stream=True,\n",
       "):\n",
       "    print(message.choices[0].delta.content, end=\"\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 5. Use Microsoft TTS API for Text-to-Speech\n",
       "Finally, we show how to use Microsoft's Text-to-Speech API to convert text into speech. We use an API endpoint and handle the response to get the audio output."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stdout",
        "text": [
         "Text-to-Speech response received.\n"
        ]
       }
      ],
      "source": [
       "API_URL = \"https://api-inference.huggingface.co/models/microsoft/speecht5_tts\"\n",
       "headers = {\"Authorization\": \"Bearer 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\"}\n",
       "\n",
       "def query(payload):\n",
       "    response = requests.post(API_URL, headers=headers, json=payload)\n",
       "    return response.content\n",
       "\n",
       "audio_bytes = query({\n",
       "    \"inputs\": \"How are you?\",\n",
       "})\n",
       "\n",
       "# Display the audio\n",
       "Audio(audio_bytes)"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   