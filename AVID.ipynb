{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04e7a6e",
   "metadata": {},
   "source": [
    "# Speech Detection and Transcription with WebRTC VAD and Whisper\n",
    "\n",
    "This notebook demonstrates how to use WebRTC Voice Activity Detection (VAD) to check if an audio file contains speech and then transcribe it using the Whisper model. The process is broken down into several steps:\n",
    "\n",
    "1. **Load and Preprocess the Audio File**\n",
    "2. **Initialize WebRTC VAD**\n",
    "3. **Perform VAD to Detect Speech**\n",
    "4. **Transcribe the Audio Using Whisper**\n",
    "5. **Save the Transcription to a File**\n",
    "\n",
    "Let's start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99c66873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import webrtcvad\n",
    "import whisper\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2a8195",
   "metadata": {},
   "source": [
    "## Load and Preprocess the Audio File\n",
    "We first need to load the audio file and preprocess it. Whisper expects audio in a specific format, so we need to ensure that the audio is correctly formatted and preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "598b2e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = \"aud.mp3\"\n",
    "\n",
    "# Check if the file exists and read it\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = file.read()\n",
    "else:\n",
    "    print(\"File not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc4543e",
   "metadata": {},
   "source": [
    "Next, we'll load and preprocess the audio file using Whisper. This includes converting it into a format suitable for the VAD and Whisper models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c8debf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Whisper model\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Load and preprocess audio file\n",
    "audio = whisper.load_audio(file_path)\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "\n",
    "# Convert audio to raw PCM bytes for VAD processing\n",
    "audio_int16 = np.int16(audio * 32767)  # Convert float32 [-1, 1] to int16 [-32767, 32767]\n",
    "audio_pcm = audio_int16.tobytes()  # Convert to raw PCM bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6961707c",
   "metadata": {},
   "source": [
    "## Initialize WebRTC VAD\n",
    "We initialize the WebRTC VAD (Voice Activity Detection) and configure it to detect speech. VAD helps determine if the audio contains speech segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9776eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WebRTC VAD\n",
    "vad = webrtcvad.Vad()\n",
    "vad.set_mode(2)  # 0 to 3. 3 is the most aggressive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e50f72d",
   "metadata": {},
   "source": [
    "## Perform VAD to Detect Speech\n",
    "We apply VAD to the entire audio file to check if it contains speech. VAD processes the audio in small frames to determine if each frame contains speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a68c4ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform VAD on the entire audio\n",
    "sample_rate = 16000  # Whisper processes audio at 16kHz\n",
    "frame_duration_ms = 30  # Duration of each frame in ms\n",
    "frame_size = int(sample_rate * frame_duration_ms / 1000)  # Frame size in samples\n",
    "num_frames = len(audio_pcm) // (frame_size * 2)  # Number of frames\n",
    "\n",
    "contains_speech = False\n",
    "\n",
    "for i in range(num_frames):\n",
    "    frame = audio_pcm[i * frame_size * 2:(i + 1) * frame_size * 2]\n",
    "    if vad.is_speech(frame, sample_rate=sample_rate):\n",
    "        contains_speech = True\n",
    "        break\n",
    "\n",
    "# Check if the audio contains speech before transcribing\n",
    "if contains_speech:\n",
    "    print(\"Speech detected in the audio.\")\n",
    "else:\n",
    "    print(\"No speech detected in the audio.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5557a5",
   "metadata": {},
   "source": [
    "## Transcribe the Audio Using Whisper\n",
    "If the audio contains speech, we proceed to transcribe it using the Whisper model. The transcription results are saved to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9337a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform transcription if speech is detected\n",
    "if contains_speech:\n",
    "    # Perform transcription\n",
    "    options = whisper.DecodingOptions(language=\"en\", task=\"transcribe\")\n",
    "    result = whisper.decode(model, mel, options)\n",
    "    text_output = result.text\n",
    " \n",
    "    # Save the transcription to a text file\n",
    "    with open(\"transcription.txt\", \"w\") as file:\n",
    "        file.write(result.text)\n",
    "    \n",
    "    print(\"Transcription saved to 'transcription.txt'.\")\n",
    "else:\n",
    "    print(\"No speech detected in the audio.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c916fc",
   "metadata": {},
   "source": [
    "## Alternative Transcription Method\n",
    "Additionally, you can use the Whisper model directly to transcribe the audio file. This method bypasses VAD and directly processes the audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97e4824f",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stdout"
    }
   ],
   "source": [
    "# Alternative transcription method\n",
    "model = whisper.load_model(\"tiny\")\n",
    "result = model.transcribe(file_path)\n",
    "print(\"Transcription:\")\n",
    "print(result[\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
